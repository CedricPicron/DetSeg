Encoder:
    - Profiling (training speed on 1050):
        # softmax (backward):
            * slowest component (approx 4.0 ms)
            * cannot be removed for now
        # masked_fill (forward and backward):
            * second (forward) and third (backward) slowest components (both approx 3.4 ms)
            * masked_fill_ does not provide a solution as slower in backward pass (copy needed)
            * masked_fill could be removed though (network should learn to ignore padded areas)

    - Profiling (training memory):
        # dropout (in attention):
            * highest memory consumption (80 Mb)
            * can be reduced by using inplace dropout (16 Mb reduction)
            * inplace is slower though (as not fused) and therefore not worth it

Some ideas:
    1. Some in-between models could be designed. For example a decoder with global attention
       and also enhance with the segmentation and curiosity part. Conversely, one could also
       design a decoder with sampling but without the segmentation and curiosity part.
    2. If one wants the increase the number of samples per slot or the number of slots, it is
       more efficient to first compute the keys and values, and then sample the correct ones.
       Otherwise, in the default multi-head attention implemention, these keys and values are
       recomputed many times due many samples being sampled by many different slots.
    3. Note that the Facebook global decoder adds position encodings (a.k.a. slot embeddings),
       to the slots at every cross and self-attenion module. Note that currently no such thing 
       is done in the sample decoder. Additionally, they also add a final LayerNorm layer at
       the end of the decoder. Also something which is currently not done in the SampleDecoder.
    4. Currently no object removal mechanism is implemented. However, when such mechanism is
       added, one should also change the loss, as currently no loss is attributed to unmatched
       targets. One could therefore get zero loss when no predictions are made and where all 
       targets are unmatched. In such a case, one should recover the slot that was spawned the
       closest to each unmatched target, and use this as matched prediction. Backpropagating
       losses w.r.t. to these additional prediction-target pairs should allow the network to 
       learn not to remove these predictions (i.e. slots) in the future.
    5. Currently the GIoU loss is computed by comparing every selected prediction with every 
       target, followed by taking the diagonal of this GIoU matrix. A more efficient solution
       would only compare each selected prediction with its corresponding target, avoiding
       some unnecessary computation. In practice however, it seems that computing the full
       GIoU matrix is not that expensive, both on time and memory consumption. For simplicity,
       we therefore keep it as is.
    6. The current version supports auxiliary losses. It however assumes that the number of
       slots remains constant across the different prediction sets. Given that currently no
       object removal and addition mechansims are implemented, this is not a problem. However,
       when such mechanisms are added, one should take care of this limitation. Note that one
       can alternatively opt for a solution where each addition is accompanied with a removal,
       such that the total number of object slots always remains the same.